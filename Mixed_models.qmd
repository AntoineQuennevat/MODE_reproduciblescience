---
title: "Mixed models in ecology"
bibliography: references.bib
execute: 
  freeze: auto
output: 
  html_document:
   toc: true
   toc_float: true
editor: 
  markdown: 
    wrap: sentence
---

```{r Importation des packages,fig.align='center'}
library(nlme)
library(ggplot2)
library(gridExtra)
library(predictmeans)
library(sp)
```


## Introduction

The chapter is based on the "Modèle mixte, modélisation de la variance" section of the course "Le Modèle Linéaire et ses Extensions" (L. Bel et al. 2016), uses its examples to illustrate the various aspects of mixed models.

**General Linear Models**, such as linear regressions, ANOVA, and ANCOVA, are commonly employed to depict the relationships between a dependent variable, denoted as (Y), and one or more independent variables ((X_1, X_2, ..., X_n)).
These models are based on several assumptions, including homoscedasticity of the variance, non-collinearity of residuals, and normality of residuals.
Generalized Linear Models (GLMs) can address homoscedasticity and normality assumptions by accommodating data from different distributions like Poisson, binomial, or Gamma distributions, which are often encountered in ecology.
However, it is crucial to validate the non-collinearity of residuals.

In biological and ecological experiments, the assumption of independence of measurements, necessary for non-collinearity of residuals, is frequently violated.
This is because measurements are often correlated within families, regions, repeated on the same individuals, or across time and sites.
In such cases, it becomes necessary to employ **mixed models**.
These models, extensions of both general and generalized linear models, consider the correlation of measurements by introducing individuals, regions, families, or other factors as **random effects** in the models.
This incorporation allows for a more accurate representation of the complex dependencies present in the data.

*What is a random effect, and how do I determine if my effect is random or fixed ?*

To clarify the distinction between fixed and random effects, let's examine two examples:

-   **Example 1: Comparing individual cars**

    Abdel, Antonio, Odeline, and Aela want to compare the oil consumption of their individual cars.
    They conduct a test by measuring oil consumption during a 30-kilometer drive, repeated five times in a day, with consistent traffic conditions and driving patterns.
    The dataset consists of one factor with four levels (representing the four cars) and five replicates each.
    Performing a one-way ANOVA allows them to determine which car is the most economical.
    In this scenario, the factor "car" is **fixed**, and the analysis provides conclusions specific to the four studied cars.

-   **Example 2: Assessing homogeneity within a car model**

    A car constructor aims to evaluate the homogeneity of oil consumption within a car model, treating the model as a population of cars with expected heterogeneity in gas consumption.
    Similar to Example 1, they measure oil consumption by driving each car 30 kilometers, five times in a day, resulting in a dataset with one factor and four levels, each with five replicates.
    Unlike the first example, the cars in this case were sampled from a larger population, and the objective is to draw conclusions about the entire population, not just the sampled cars.
    Here, a mixed model with the factor 'car' as a **random factor** should be used.

In summary, a factor is designated as fixed when the experimenter intentionally chooses a limited number of levels for investigation, aiming to assess the impact of each level on the response variable.
On the other hand, a factor is considered random when the selected levels represent only a sample from all possible levels.
In this case, the objective is to understand the variability in the response variable attributed to this factor.

For example, let's consider a researcher investigating the influence of the number of training sessions per week on the concentration of red blood cells in recreational athletes.
The researcher collects data from 50 athletes in a local club who train between 1 and 5 times a week.
Initially planning a simple ANOVA with the number of training sessions as the main factor, the researcher discovers that most athletes in the dataset belong to only 10 families, leading to non-independent measurements.
To address this issue, the researcher opts for a mixed model, treating the number of training sessions as a fixed factor and the family as a random factor.
This approach allows the exploration of variability between families without the intention of directly comparing them.

Now that we have a general understanding of what mixed models are, we can delve into the mathematical formalism of these models.
In this chapter, you will discover how **matrices** can be employed to create mixed models, explore the various **dependency structures** that exist, and ultimately, find an **implementation** of mixed models in R.

## 1 The model

The linear mixed model can be formulated as follows:

$$
Y_{i} = \beta X_{i} + \gamma_i Z_{i} + \varepsilon_{i}
$$

where:

-   $Y_i = n_i \times 1$ measurements for subject $i$, where $n$ is the numer of observations
-   $X_i = n_i \times p$ matrix of vectors for fixed effects, where $p$ is the number of parameters for fixed effects.
-   $\beta_i= p \times 1$ parameters for fixed effects
-   $Z_i = n_i \times p$ matrix of vectors for random effects
-   $\gamma_i = r \times 1$ parameters for random effects, where $r$ is the number of parameters for random effects.
-   $\varepsilon_i = n_i \times 1$ residuals for individual $i$

A mixed effects model incorporates **random effects** ($\gamma_i$), or a combination of both **random and fixed effects** ($\beta$), whereas a standard linear model includes only fixed effects.
The distinction between these two lies in the nature of the effects related to the treatment under study.

When it is evident that the researcher intends to compare particular, predefined levels of a treatment, those levels are considered fixed effects.
Conversely, when the levels of the treatment are drawn from a larger population of possible levels, the treatment is treated as a random effect.

In addition, random effects are included in a model when there is a correlation or dependence among the observations that cannot be ignored.

**RANDOM VARIABLE** = « *something that could not be known before sampling/measurement/observation*".


In matrix form, the mixed model is written as:

$$
Y \sim \mathcal{N{n}}(X\theta, \Sigma)
$$ where:

$Y$ is the response vector of the observations, $X\theta$ is the expectation of the response vector $Y$ and $\Sigma$ is the variance matrix.

We can note that if the response vector $Y$ is of dimension $n$, the matrix $\Sigma$ is of dimensions $n \times n$.
Since $\Sigma$ is symmetric, it comprises $n(n + 1)/2$ parameters.
This is because, in a symmetric matrix, the elements above (or below) the main diagonal are the same as those below (or above), reducing the total number of parameters needed to describe the matrix.

However, the limitation of the available data prevents considering models where all these $n(n + 1)/2$ parameters are free.
This restriction arises from the need to have a significant amount of data to reliably estimate each parameter, which quickly becomes unrealistic with a limited dataset.

To address this issue, the linear mixed-effects model proposes an approach where a structure is imposed on the variance matrix $\Sigma$.
This structure, governed by a limited number of parameters called "variance parameters," denoted $\psi$, reduces the number of parameters needed to describe the covariance matrix.
Consequently, the model can be realistically adapted even with a limited amount of data, while accounting for the correlation between observations within the framework of linear mixed-effects models.
The model parameters include $\theta$ for the expectation and $\psi$ for the variance.

## 2 Matrix computation

It is possible to encounter (mixed) linear models written under their matricial form, for their concision. It is therefore natural to present this form in the context of mixed models.

As a reminder, a linear model, like linear regression, with $p$ explanatory variables can be written $$y_i = \beta_0 + \beta_1x_i^{(1)} + \ldots + \beta_px_i^{(p)} + \varepsilon_i$$, where $y_i$ represent an observation of the response variable $Y$ for the individual $i$, $\beta_0$ the intercept, $\beta_1,...,\beta_p$ the coefficients associated to each explanatory variable $X_1,...,X_p$, $x_i^{(1)},...,x_i^{(p)}$ the $p$ observations (for the $p$ explanatory variables) for the individual $i$, and $e_i$ an error term associated to the individual $i$.
We can see $e_i$ as a realization of a random variable $E_i$ 
distributed according to a normal law $\mathcal{N}(0,\sigma^2)$.
Noting $$y=\begin{pmatrix}
y_1\\
\vdots\\
y_n\\
\end{pmatrix}$$, $$X=\begin{pmatrix}
1&x_1^{(1)} & \ldots & x_1^{(p)}\\
\vdots & \vdots & \ldots & \vdots \\
1 & x_n^{(1)} & \ldots & x_n^{(p)}
\end{pmatrix}$$, $$\theta=\begin{pmatrix}
\beta_0\\
\vdots\\
\beta_p\\
\end{pmatrix}$$ and $$e=\begin{pmatrix}
\varepsilon_1\\
\vdots\\
\varepsilon_n\\
\end{pmatrix}$$, we can rewrite the previous model under the form $$y=X\theta+e$$.
Here, $e$ is a vector of $n$ independant realizations od a random variable $E_i$ following a normal distribution $\mathcal{N}(0,\sigma^2)$.
Hence, $e$ is a realization of a random variable $E$ following the distribution $\mathcal{N}_n(0,\sigma^2I_n)$ ($e_i$ is an observation of the random variable $E_i$ distributed according to a normal law $\mathcal{N}(0,\sigma^2)$).
Similarly, $y$ is an observation of $Y=X\theta+E$ where $Y\sim\mathcal{N}_n(X\theta,\sigma^2I_n)$ ($y_i$ is an observation of $Y_i$ distributed according to a normal law $\mathcal{N}((X\theta)_i,\sigma^2)$).
Hence, by introducing $Y$ and $E$, the previous model can be written $Y=X\theta+E$ where $\mathrm{E}\stackrel{iid}\sim\mathcal{N}_n(0,\sigma^2I_n)$.

By definition, the mean response is equal to $X\theta$, more or less an error term equals to 0 in average butt that varies of $\sigma^2I_n$.
Thus, we can write $$Y\sim\mathcal{N}_n(X\theta,\sigma^2I_n)$$.
We note that when writing $\mathrm{E}\stackrel{iid}\sim\mathcal{N}_n(0,\sigma^2I_n)$, each error has got the same variance ($\sigma^2$) because every samples are independant.
We will see subsequently that if this independence condition is not respected, all the errors do not have the same variance, we speak of a dependence structure.The dependence between the measurements determines the dependence structure (measurements repeated over time, individuals grouped by common ancestry, etc.).
We will see here that in the context of mixed models, the previous equation is written $$Y\sim\mathcal{N}_n(X\theta,\sum)$$.
When writing this, we note that the average response does not change. Generalization concerns errors.
Now, let's study study the components of variance $\sum$ by following an example. We decide to study the heritability of a trait (height for example). We want to see if individuals from one ascendant are more similar than those from another ascendant. We have $m$ ascendants, numbered $i=1,...,m$, from a larger population, each having $n$ descendants numbered $j=1,...,n$. We pose $Y_{ij}=$ the trait value for the j-ème descendant of the i-ème ascendant.
Individuals with the same ascendant are therefore grouped by their belonging to the same ascendant. The fact of sampling ancestors from a larger population (random effect) introduces a correlation between the traits measured on the descendants of the same ancestor. This correlation is uniform between individuals. The descendants between the groups (for different ancestors) are independent. Thus, we write $$
\mathbb{Cov}(Y_{ij},Y_{i'j'}) = \left\{
    \begin{array}{ll}
        \gamma^2 & \mbox{si } i=i'\\
        0 & \mbox{sinon.}
    \end{array}
\right.
$$
This model therefore includes a variance associated with the ascendant effect $\gamma^2$ and a residual variance $\sigma^2$. We precise that $\gamma^2$ represent the variability between the ascendants.
Starting from the linear model $Y=X\theta+E$ where $E$ is still $\sigma^2I_n$, we just have to take into account this variance associated with the ascendant effect. To do this, we introduce the matrix $Z$, of dimension $n\times m$, where $$
Z_{a,i} = \left\{
    \begin{array}{ll}
        1 & \mbox{l'individu } a=(i,j) \mbox{ est le descendant de l'ascendant }i\\
        0 & \mbox{sinon.}
    \end{array}
\right.
$$ and the vector $U$ of dimension $m$ including the random effects $\gamma^2$. 
Hence, the previous model can be rewritten $Y=X\theta+ZU+E$. We remind that $U$ and $E$ are independants, gaussiens centered, and that $\mathbb{Var}(U)=\gamma^2I_m$.
Consequently, $\mathbb{E}(Y)=X\theta$ and $\sum = \mathbb{V}(Y)=Z\mathbb{V}(U)Z'+\mathbb{V}(E) = \gamma^2ZZ'+\sigma^2(I_n)$.
We get back to the matricial form $Y\sim\mathcal{N}(X\theta,\sum)$ with $$\sum=\begin{pmatrix}
R&0 & \ldots & 0\\
0 & \ddots & \ddots & 0 \\
\vdots & \ddots & \ddots & 0 \\
0 & \ldots & 0 & R
\end{pmatrix}$$, where $$R=\begin{pmatrix}
\sigma^2+\gamma^2&\gamma^2 & \ldots & \gamma^2\\
\gamma^2 & \ddots & \ddots & \gamma^2 \\
\vdots & \ddots & \ddots & \gamma^2 \\
\gamma^2 & \ldots & \gamma^2 & \sigma^2+\gamma^2
\end{pmatrix}$$. $R$ corresponds to the representation of the variances due to the random effect and residuals. Note that the diagonal blocks are of the same dimensions if the number of descendants is identical for each ascendant.


## 3 Dependency structure

### 3.1 Case of repeated measurements

We want to evaluate the effect of different diet on weight gain in champanzee.
Several animals $j$ follow each diet $i$ and they kept the same diet accross all the experiment.
Each week animal weight ($Y_{ij}$) is measured, during T weeks.
In this case, measures are repeated accross time, such measurements are called *longitudinal data*.
To analyse this data, the temporal dependency must be taken into account, for this, the following can be used:

$$E(Y_{ijt}) = µ + α_i + γ_t + (αγ)_{it}$$

with

$$Cov(Y_{ijt}, Y_{i'j't'}) = \left\{ 
  \begin{array}{ll}
  σ^2ρ \ \ si \ \ (i, j) = (i', j') \\
     0 \ sinon \
   \end{array}
  \right.$$

In this model, the covariance between two measurements made at times t and t' on the same model is constant, whatever the time interval between the two measurements.

**Model** A model is proposed here which takes into account the kinetic aspect of the experiment and predicts that the dependence between two measurements depends on the time interval between them.
Such a dependency cannot be represented simply as a random effect.
We therefore assume that the weights are Gaussian with respective expectations.

$$ E(Y_{ijt}) = µ + α_i + γ_t + (αγ)_{it} $$ **Dependency structure** It is also assumed that all measurements have the same variance

$$ V(Y_{ijt} =  σ^2) $$ and the covariance between them is

$$Cov(Y_{ijt}, Y_{i'j't'}) = \left\{ 
  \begin{array}{ll}
  σ^2ρ^{|t-t'|} \ \ si \ \ (i, j) = (i', j') \\
     0 \ sinon \
   \end{array}
  \right.$$

This structure assumes that measurements made on different animals are independent.
It is also assumed that \|ρ\| \< 1, which implies that the longer the time interval, the less correlated the tests on the same animal.
This form of covariance corresponds to an autoregressive process of order 1, generally denoted AR(1).
This model has two variance parameters (also called variance components): the temporal correlation ρ and the variance of each observation $σ^2$.

$$
  ψ = \left( {\begin{array}{cc}
    ρ \\
    σ^2 \\
  \end{array} } \right)
$$

Because of the independence between the measurements obtained on different animals, the variance matrix Σ also has the same diagonal block shape, but the R block differs.
:

$$
  R = \left( {\begin{array}{cc}
     σ^2 & σ^2ρ & σ^2ρ^2 & ... & σ^2ρ^{T-1}\\
     σ^2ρ & ... & ... & ... & ...\\
     σ^2ρ^2 & ... & σ^2 & ... & σ^2ρ^2\\
     ... & ... & ... & ... & σ^2ρ\\
    σ^2ρ^{T-1} & ... & σ^2ρ^2 & σ^2ρ & σ^2\\
  \end{array} } \right)
$$

```{r}
#Read the data
d=read.delim(
"http://dnett.github.io/S510/RepeatedMeasures.txt")
#Create Factors
d$Program = factor(d$Program)
d$Subj = factor(d$Subj)
d$Timef = factor(d$Time)

```

```{r}
o.ar1 = gls(Strength ~ Program * Timef, data = d,correlation = corAR1(form = ~1 | Subj))
 
```

```{r}
summary(o.ar1)
```

### 3.2 Case of spatial autocorrelation

*Dependency structure* We want to take into account the dependency due to the possible spatial proximity between the sites at which the measurements were taken.

To do this, d(i, i') is the distance separating sites i and i', and the following equation is used

$$Cov(Y_i, Y_i{'}) = e^{−δ.d(i,i')}$$

As in the case of repeated measurements, there is no simple way of writing this in terms of random effects.
Moreover, since all the measurements are dependent, the matrix Σ is no longer diagonal per block and is written as :

$$
  Σ = \left( {\begin{array}{cc}
     σ^2 + γ^2 & e^{−δ.d(i,i')}& ...& e^{−δ.d(i,i')}\\
     e^{−δ.d(i,i')}& σ^2 + γ^2 & e^{−δ.d(i,i')} & \vdots\\
     \vdots & e^{−δ.d(i,i')}  & σ^2 + γ^2 & e^{−δ.d(i,i')}\\
 e^{−δ.d(i,i')}& \ldots & e^{−δ.d(i,i')} & σ^2 + γ^2\\
  \end{array} } \right)
$$

```{r, include=FALSE, echo=FALSE}
set.seed(3)  
n <- 100

loc <- data.frame(LAT = runif(n, 144, 150), LONG = runif(n,
    -26, -20))


grid <- expand.grid(LAT = seq(144, 150, l = 100), LONG = seq(-26,
    -20, l = 100))
coordinates(grid) <- ~LAT + LONG


# Set up distance matrix
distance <- as.matrix(dist(as.data.frame(loc)))  #* 1/min(distance[lower.tri(distance)])
# Generate random variable
delta <- 0.5
Vmat <- function(n, mu = 0, V = matrix(1)) {
    p <- length(mu)
    if (any(is.na(match(dim(V), p))))
        stop("Dimension problem!")
    D <- chol(V)
    t(matrix(rnorm(n * p), ncol = p) %*% D + rep(mu,
        rep(n, p)))
}
V <- Vmat(1, rep(0, n), exp(-delta * distance))
x <- rnorm(n, 30, 10)
# image(cbind(simgrid, V))
y <- 50 + 1 * x + 60 * V  #+ rnorm(n,0,1)
data.spatialCor <- data.frame(y, x, loc)
```

```{r}
data.spatialCor.glsExp <- gls(y ~ x, data = data.spatialCor,
    correlation = corExp(form = ~LAT + LONG, nugget = TRUE),
    method = "REML")
```

```{r}
summary(data.spatialCor.glsExp)
```

In spatially correlated data, variance increases with increasing distance up to a point *the sill*.
The span of distances over which points are correlated is called *the range*.

While we might expect the value of variance at a distance of zero to be zero, in reality we rarely have sampling units that approach such a small distance from one another.
The value of variance when distance is equal to zero is *the nugget*.
Typically this is the result of unexpected variability in your data that spatial patterns alone cannot account for.

Here, in our example, the value of the sill, the range and the nugget are respectively 47.68, 1.69 and 0.12.

## 4 Application

Pour cet exemple d'application de modèle mixte, nous allons utiliser un modèle mixte linéaire général.
C'est un cas particulier de modèle linéaire général, dans lequel la réponse est quantitative et les variables prédictives sont à la fois quantitatives et qualitatives, et le modèle inclut des facteurs aléatoires pour tenir compte de la dépendance des données.
Les modèles mixtes doivent respecter la normalité des résidus et l'homogénéité des variances.

----\> Explications du jeu de données pinguins, présenter les variables (Ici, pourquoi pas expliquer le body mass des pinguins en fonction de species, sex et island, avec year en facteur aléatoire - anova modèle mixte)

----\> Pas équilibré le nombre d'individus par niveaux pour les $X$, important ???
----\> Est-ce que les facteurs sont croisés (pour les interactions entre les $X_s$) ???
----\> Expliquer le graphe (pour les interactions entre les $X_s$) ----\> Cites Yannick Outreman pour la structure et les lignes de code

### 4.1 Data import

----\> Pas équilibré le nombre d'individus par niveaux pour les $X$, important ???
----\> Est-ce que les facteurs sont croisés (pour les interactions entre les $X_s$) ???
----\> Expliquer le graphe (pour les interactions entre les $X_s$) ----\> Cites Yannick Outreman pour la structure et les lignes de code

```{r}
# Importation des données
df <- read.table("https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv", sep = "," , header = TRUE)
# Bien s'assurer que nos variables 'species', 'island' et 'sex' sont des facteurs
df$species=as.factor(df$species)
df$island=as.factor(df$island)
df$sex=as.factor(df$sex)

# On vérifie qu'il n'y a pas de valeurs manquantes
colSums(is.na(df))
```

On voit qu'il y a des valeurs manquantes, dont 2 pour la variable réponse $Y$ "body_mass_g' et 11 pour la variable explicative $X$ 'sex'. On va supprimer les lignes qui présentes les valeurs manquantes.

```{r}
# On repère les lignes qui présentes les valeurs manquantes.
which(is.na(df$body_mass_g), arr.ind=TRUE)
which(is.na(df$sex), arr.ind=TRUE)
#On supprime les lignes 4, 9, 10, 11, 12, 48, 179, 219, 257, 269, et 272.
df=df[-c(4,9,10,11,12,48,179,219,257,269,272), ]

# On vérifie qu'il n'y a pas de valeurs manquantes
colSums(is.na(df))
#Il n'y a plus de valeurs manquante.
```

### 4.2 Data exploration

Avant toute analyse statistique, il est INDISPENSABLE d'explorer les données afin d'éviter toute erreur.
Voici la liste des explorations à effectuer avant la modélisation :

1.  Vérifier la présence de valeurs aberrantes dans $Y$ et la distribution des valeurs de $Y$.
2.  Si $X$ est une variable quantitative indépendante, vérifier la présence de valeurs aberrantes dans X et la distribution des valeurs de X. 2b. 2b. Si $X$ est une variable indépendante qualitative, analyser le nombre de niveaux et le nombre d'individus par niveau.
3.  Analyser les relations potentielles entre $Y$ et les $X_{s}$.
4.  Vérifier la présence d'interactions entre $X_{s}$.
5.  Vérifier la présence de colinéarité entre $X_{s}$.

#### 4.2.1 Outliers in $Y$ and $Y$ distribution

```{r datahist, include=TRUE, fig.height=5, fig.width=6}
par(mfrow=c(2,2))
# Boxplot
boxplot(df$body_mass_g,col='blue',ylab='Masse corporel')
# Cleveland plot
dotchart(df$body_mass_g,pch=16,col='blue',xlab='Masse corporel')
# Histogram
hist(df$body_mass_g,col='blue',xlab="Masse corporel",main="")
# Quantile-Quantile plot
qqnorm(df$body_mass_g,pch=16,col='blue',xlab='')
qqline(df$body_mass_g,col='red')
```

Ici, le Boxplot et le Cleveland Plot nous permettent de voir qu'il n'y a pas d'individus présentant des valeurs aberrantes.
Le Cleveland Plot nous montre qu'il semble y avoir un groupe d'individus qui présentent une masse corporel entre 5000 et 6000g, alors que le reste du groupe se situe entre 3000 et 4000g.
L'Histogramme et le QQ Plot nous montre que $Y$ suit difficilement un loi Normale... Ce n'est pas très grave, car la validité de modèle se base entre autre sur la normalité des résidus, que l'on démontrera par la suite.

#### 4.2.2 Outliers in $Xs$

-   Pour les $Xs$ qui sont quantitatifs : vérifier les valeurs aberrantes et la distribution

Pas de prédicteur quantitatif ici.

-   Pour les $Xs$ qui sont catégoriques : nombre de niveaux et nombre d'individus par niveau

```{r datafact, include=TRUE}
# Factor Species
summary(df$species)
# Factor Island
summary(df$island)
# Factor Sex
summary(df$sex)
```

La variable 'species' présente 3 niveaux : Adelie, Chinstrap et Gentoo.
Le nombre d'individus entre les 3 niveaux n'est pas équilibré, avec moins d'individus pour l'espèce Chinstrap.
La variable 'island' présente 3 niveaux : Biscoe, Dream et Torgersen.
Le nombre d'individus entre les 3 niveaux n'est pas équilibré, avec moins d'individus pour l'île Torgersen.
La variable 'sex' présente 2 niveaux : female et male.
Le nombre d'individus par niveau s'approche de l'équilibre.

#### 4.2.3 Analysis of potential relationships Y vs Xs

Nous pouvons analyser graphiquement les relations possibles entre Y et X.
Attention, cette analyse graphique des relations entre Y et X **ne prédit en aucun cas l'importance de la relation**.
La modélisation statistique reste le seul moyen d'identifier les relations.

```{r datagraph, include=TRUE, fig.height=4, fig.width=6}

par(mfrow=c(1,1))
# Espèces
plot(df$body_mass_g~df$species,pch=16,col='blue',xlab='Espèces',ylab='Masse corporel en g')

# Îles
plot(df$body_mass_g~df$island,pch=16,col='blue',xlab='Îles',ylab='Masse corporel en g')

# Sexe
plot(df$body_mass_g~df$sex,pch=16,col='blue',xlab='Sexe',ylab='Masse corporel en g')
```

COncernant l'espèce, on peut voir que Gentoo présente une masse corporel plus élevée (entre 5000 et 6000g) que les deux autres espèces (entre 3000 et 4000g).
Concernant les îles, on peut voir que les individus présents sur Biscoe présentent une masse corporel plus élevée (entre 5000 et 6000g) que les individus présents sur les deux autres îles (entre 3000 et 4000g).
Enfin, concernant le sexe, les mâles semblent présenter une masse corporel un peu plus importante que les femelles.

#### 4.2.4 Analysis of possible interactions between the two independent variables

Ici, nous allons considérer l'interaction entre les trois facteurs étudiés.
Pour estimer la présence d'effets interactifs, nous développons une approche graphique.
Rappelons que l'interaction entre deux facteurs ne peut être testée que si les facteurs sont croisés (c'est-à-dire que tous les niveaux d'un traitement sont représentés dans tous les niveaux de l'autre traitement et réciproquement = un plan factoriel).
Ce point doit être testé avant.

```{r dataInterFac, include=TRUE, fig.height=4, fig.width=7}

# Les facteurs sont croisé ? Dépend du design experimental

# Interaction Species:Island:Sex
par(mfrow=c(1,1))
boxplot(df$body_mass_g~df$species*df$island*df$sex, varwidth = TRUE, xlab = "Espèces.Îles.sexe", ylab = "Masse corporelle", col='blue2', main = "")
```

Explications

#### 4.2.5 Check collinearity between X

La colinéarité fait référence à la situation dans laquelle deux ou plusieurs variables prédictives de colinéarité sont étroitement liées les unes aux autres.
La présence de colinéarité peut poser des problèmes dans le contexte de la régression, car il peut être difficile de séparer les effets individuels des variables colinéaires sur la réponse.

Ici, nous allons tester la colinéarité entre nos 3 variables prédictives :

```{r col, include=TRUE, fig.height=4, fig.width=7}
# ploting Species by Island
ggplot(df, aes(x=species, y=island)) +
  geom_point() +
  theme_bw() -> p1

# ploting Species by Sex
ggplot(df, aes(x=species, y=sex)) +
  geom_point() +
  theme_bw() -> p2

# ploting Island by Sex
ggplot(df, aes(x=island, y=sex)) +
  geom_point() +
  theme_bw() -> p3

# Ploting side-by-side
marrangeGrob(list(p1,p2,p3), nrow=1, ncol=3, top=NULL)
```

On peut voir dans notre exemple que pour l'intéraction entre Species et Sex, il y a bien les deux modalités du sexe par espèces, et pour l'intéraction entre Island et Sex, qu'il y a bien les deux modalités du sexe par îles.
Seulement, on peut remarquer que pour l'intéraction entre Species et Island, toutes les îles ne comportent pas toutes les espèces!
On ne pourra donc pas tester l'influence des îles et des espèces aux vues de ce résultat.
On décide donc par la suite de supprimer la variable Island de notre analyse.
On testera l'influence de l'espèce et du sexe sur la masse corporelle des pinguins, avec toujours les années en effet aléatoire.

### 4.3 Statistical analysis

#### 4.3.1 Model construction

Pour la modélisation statistique, nous analysons d'abord le modèle complet (modèle contenant toutes les variables indépendantes à tester).

Pour obtenir le modèle candidat (modèle ne contenant que les termes significatifs) à partir du modèle complet, nous utiliserons le **MODÈLE DE 'BACKWARD SELECTION'**, c'est-à-dire la sélection du modèle basée sur la significativité des termes.
Selon cette approche, on commence par créer le modèle complet avec toutes les variables d'intérêt, puis on abandonne la variable la moins significative, tant qu'elle n'est pas significative.
Nous continuons en réajustant successivement des modèles réduits et en appliquant la même règle jusqu'à ce que toutes les variables restantes soient significatives.
La suppression des termes non significatifs doit suivre les deux étapes suivantes: - Premièrement, on supprime successivement les interactions non significatives.
- Deuxièmement, on supprime successivement les effets principaux non significatifs.
Un effet principal n'est supprimé que s'il est non significatif ET s'il n'est pas contenu dans une interaction significative.

Dans cet exemple, on considère une mesure de dépendance au niveau des années (par ex., une mesure de la masse faite en 2009 dépend de la mesure faite en 2008 qui elle même dépend de la mesure réalisée en 2007).
La présence de l'effet aléatoire de l'année s'intégrera non pas avec la fonction lm, mais lme (du package nlme).

```{r anova, include=TRUE}
# Modèle complet
mod1=lme(body_mass_g~species
              + sex
              + species:sex
              ,random=~1|year
              ,data=df)

# Then we check for significance
#anova(mod1)

#Anova Output
#            numDF denDF  F-value p-value
#(Intercept)     1   325 61569.82  <.0001
#species         2   325   758.36  <.0001
#sex             1   325   387.46  <.0001
#species:sex     2   325     8.76   2e-04
```

On peut voir dans l'output de l'anova de notre modèle complet que chaque interactions et chaque variable est significativce (\<0.05).
Le modèle complet est donc le modèle candidat.

#### 4.3.2 Model's coefficients analysis

```{r coeffm, ,include=TRUE}
# Coefficients of the model
summary(mod1)

# The output is:
#Fixed effects:  body_mass_g ~ species + sex + species:sex 
#                            Value Std.Error  DF  t-value p-value
#(Intercept)              3368.836  36.21222 325 93.03036  0.0000
#speciesChinstrap          158.370  64.24029 325  2.46528  0.0142
#speciesGentoo            1310.906  54.42228 325 24.08767  0.0000
#sexmale                   674.658  51.21181 325 13.17387  0.0000
#speciesChinstrap:sexmale -262.893  90.84950 325 -2.89372  0.0041
#speciesGentoo:sexmale     130.437  76.43559 325  1.70650  0.0889


```

A partir de cette table, on peut déterminer les coefficients du modèle tels que :

**Species factor**\
- $species_{Adelie}$ = 0 (the baseline of the factor Habitat) - $Species_{Chinstrap}$ = $158.370$ - $Species_{Gentoo}$ = $1310.906$

**Sex factor**\
- $Sex_{female}$ = 0 (the baseline of the factor Habitat) - $Sex_{male}$ = $674.658$

\*\*Interaction\*\
- $Species_{Chinstrap}$:$Sex_{male}$ = $-262.893$ - $Species_{Gentoo}$:$Sex_{male}$ = $130.437^{NS}$

So, the candidate model is: $$  Species = 3369 + (Adelie = 0, Chinstrap = 158, Gentoo = 1311)  + (Female = 0,\: Male = 675)$$ $$       + (Adelie_{Male} = 0, \:Chinstrap_{Male} = -263,\: Gentoo_{Male} = 130^{NS}) $$

For sake of simplicity, we can write the model depending on the sexe :

The model for the *Female* pinguin is : $$ Sex_{Female} = 3369\:  + (Adelie = 0,\: Chinstrap = 158,\: Gentoo = 1311)$$

The model for the *Male* pinguin is : $$Sex_{Male} = 4043\: + (Adelie = 0,\: Chinstrap = - 105,\: Gentoo = 1441)$$

Ainsi, le sexe, l'espèce et l'interaction de ces deux variables (sauf entre Male et Gentoo) ont bien un impact significatif sur la masse corporelle des pingouins.
Par exemple, chez les pingouins Adélie, la femelle va avoir une masse corporelle de 3369g, alors qu'un mâle aura une masse corporelle de 4043g et chez les Chinstrap, la femelle va avoir une masse corporelle de 3527g, alors qu'un mâle aura une masse corporelle de 3938g.

### 4.4 Model validation

Pour valider le modèle, il faut : - Valider la normalité des résidus - Histogramme et QQplot des résidus - Valider l'homogénéité des variances - En addition, vérifier la présence d'observations qui auraient trop de contribution dans le modèle

#### 4.4.1 Normality of the residuals

```{r ResidNorm, include=TRUE, fig.height=3, fig.width=6}
par(mfrow=c(1,2))
# Histogram
hist(mod1$residuals,col='blue',xlab="residuals",main="Check Normality")
# Quantile-Quantile plot
qqnorm(mod1$residuals,pch=16,col='blue',xlab='')
qqline(mod1$residuals,col='red')
```

On voit bien que l'histogramme suit une loi normale, et les points du quantile plot suivent bien la ligne rouge : la normalité des résidus est validée.

#### 4.4.2 Homogeneity of the variance

```{r Residhomo, include=TRUE, fig.height=3, fig.width=9}
par(mfrow=c(1,3))

# residuals vs fitted
plot(residuals(mod1)~fitted(mod1)
      , col='blue'
      , pch=16)
abline(h = 0)

# residuals against Species
boxplot(residuals(mod1)~ df$species, 
         varwidth = TRUE,
         ylab = "Residuals",
         xlab = "Species",
         main = "")
abline(h = 0)

# residuals against Sex
boxplot(residuals(mod1)~ df$sex, 
         varwidth = TRUE,
         ylab = "Residuals",
         xlab = "Sex",
         main = "")
abline(h = 0)

```

On voit ici que pour chaque graphe, la variance des résidus est distribuée de façon homogène autour de la droite horizontale.
L'homogénéité de la variance est validée.

#### 4.4.3 Look at influential observations

```{r Contri, include=TRUE, fig.height=4, fig.width=4}
par(mfrow = c(1, 1))


CookD(mod1,newwd=TRUE)
```

On voit que l'individu 314, 315 et 325 ont une contribution un peu plus forte dans le modèle, mais ce n'est pas un résultat aberrant.
