---
title: "Mixed models in ecology"
bibliography: references.bib
execute: 
  freeze: auto
output: 
  html_document:
   toc: true
   toc_float: true
editor: 
  markdown: 
    wrap: sentence
---

```{r Importation des packages,fig.align='center'}
library(nlme)
library(ggplot2)
library(gridExtra)
library(predictmeans)
library(sp)
```

Mise en forme ::: {style="text-align: justify"} Teste :::

## Introduction

**General Linear Models**, such as linear regressions, ANOVA, and ANCOVA, are commonly employed to depict the relationships between a dependent variable, denoted as (Y), and one or more independent variables ($X_1, X_2, ..., X_n$).
These models are based on several assumptions, including homoscedasticity of the variance, non-collinearity of residuals, and normality of residuals.
Generalized Linear Models (GLMs) can address homoscedasticity and normality assumptions by accommodating data from different distributions like Poisson, binomial, or Gamma distributions, which are often encountered in ecology.
However, it is crucial to validate the non-collinearity of residuals.

In biological and ecological experiments, the assumption of independence of measurements, necessary for non-collinearity of residuals, is frequently violated.
This is because measurements are often correlated within families, regions, repeated on the same individuals, or across time and sites.
In such cases, it becomes necessary to employ **mixed models**.
These models, extensions of both general and generalized linear models, consider the correlation of measurements by introducing individuals, regions, families, or other factors as **random effects** in the models.
This incorporation allows for a more accurate representation of the complex dependencies present in the data.

*What is a random effect, and how do I determine if my effect is random or fixed ?*

To clarify the distinction between fixed and random effects, let's examine two examples:

-   **Example 1: Comparing individual cars**

    Abdel, Antonio, Odeline, and Aela want to compare the oil consumption of their individual cars.
    They conduct a test by measuring oil consumption during a 30-kilometer drive, repeated five times in a day, with consistent traffic conditions and driving patterns.
    The data set consists of one factor with four levels (representing the four cars) and five replicates each.
    Performing a one-way ANOVA allows them to determine which car is the most economical.
    In this scenario, the factor "car" is **fixed**, and the analysis provides conclusions specific to the four studied cars.

-   **Example 2: Assessing homogeneity within a car model**

    A car constructor aims to evaluate the homogeneity of oil consumption within a car model, treating the model as a population of cars with expected heterogeneity in gas consumption.
    Similar to Example 1, they measure oil consumption by driving each car 30 kilometers, five times in a day, resulting in a data set with one factor and four levels, each with five replicates.
    Unlike the first example, the cars in this case were sampled from a larger population, and the objective is to draw conclusions about the entire population, not just the sampled cars.
    Here, a mixed model with the factor 'car' as a **random factor** should be used.

In summary, a factor is designated as fixed when the experimenter intentionally chooses a limited number of levels for investigation, aiming to assess the impact of each level on the response variable.
A factor is considered random when the selected levels represent only a sample from all possible levels.
In this case, the objective is to understand the variability in the response variable attributed to this factor.

For example, let's consider a researcher investigating the influence of the number of training sessions per week on the concentration of red blood cells in recreational athletes.
The researcher collects data from 50 athletes in a local club who train between 1 and 5 times a week.
Initially planning a simple ANOVA with the number of training sessions as the main factor, the researcher discovers that most athletes in the data set belong to only 10 families, leading to non-independent measurements.
To address this issue, the researcher opts for a mixed model, treating the number of training sessions as a fixed factor and the family as a random factor.
This approach allows the exploration of variability between families without the intention of directly comparing them.

Now that we have a general understanding of what mixed models are, we can delve into the mathematical formalism of these models.
In this chapter, you will discover how **matrices** can be employed to create mixed models, explore the various **dependency structures** that exist, and ultimately, find an **implementation** of mixed models in R.

## 1. Formalization of the linear mixed model

The linear mixed model can be formulated as follows:

$$
Y_{i} = \beta X_{i} + \gamma_i Z_{i} + \varepsilon_{i}
$$

where:

-   $Y_i = n_i \times 1$ measurements for subject $i$
-   $X_i = n_i \times p$ matrix of vectors for fixed effects
-   $\beta_i= p \times 1$ parameters for fixed effects
-   $Z_i = n_i \times p$ matrix of vectors for random effects
-   $\gamma_i = r \times 1$ parameters for random effects
-   $\varepsilon_i = n_i \times 1$ residuals for individual $i$

A mixed effects model incorporates **random effects** ($\gamma_i$), or a combination of both **random and fixed effects** ($\beta$), whereas a standard linear model includes only fixed effects.

When it is clear that the researcher intends to compare particular, predefined levels of a treatment, those levels are considered fixed effects.
Conversely, when the levels of the treatment are drawn from a larger population of possible levels, the treatment is treated as a random effect.

In addition, random effects are included in a model when there is a correlation or dependence among the observations that cannot be ignored.

**RANDOM VARIABLE** = « *something that could not be known before sampling/measurement/observation*".

-   $beta_i= p \times 1$ parameters for fixed effects
-   $Z_i = n_i \times p$ matrix of vectors for random effects
-   $gamma_i = r \times 1$ parameters for random effects
-   $varepsilon_i = n_i \times 1$ residuals for individual $i$

In matrix form, the mixed model is written as:

$$
Y \sim \mathcal{N{n}}(X\theta, \Sigma)
$$ where:

$Y$ is the response vector of the observations, $X\theta$ is the expectation of the response vector $Y$ and $\Sigma$ is the variance matrix.

## 2. Matrix computation in mixed models

We can note that if the response vector $Y$ is of dimension $n$, the matrix $\Sigma$ is of dimensions $n \times n$.
Since $\Sigma$ is symmetric, it comprises $n(n + 1)/2$ parameters.
This is because, in a symmetric matrix, the elements above (or below) the main diagonal are the same as those below (or above), reducing the total number of parameters needed to describe the matrix.

However, the limitation of the available data prevents considering models where all these $n(n + 1)/2$ parameters are free.
This restriction arises from the need to have a significant amount of data to reliably estimate each parameter, which quickly becomes unrealistic with a limited dataset.

To address this issue, the linear mixed-effects model proposes an approach where a structure is imposed on the variance matrix $\Sigma$.
This structure, governed by a limited number of parameters called "variance parameters," denoted $\psi$, reduces the number of parameters needed to describe the covariance matrix.
Consequently, the model can be realistically adapted even with a limited amount of data, while accounting for the correlation between observations within the framework of linear mixed-effects models.
The model parameters include $\theta$ for the expectation and $\psi$ for the variance.

\$\$ M\_{2\times2} = \left( {

```{=tex}
\begin{array}{cc}
    A & B \\
    C & D \\
  \end{array}
```
} \right)

\$\$ Il est possible de rencontrer des modèles linéaires (mixtes) sous forme matricielle, de part la concision de la forme.
Il est donc naturel de présenter cette forme dans le cadre des modèles mixtes.

Pour rappel, un modèle linéaire, de type regression linéaire, avec $p$ variables explicatives peut s'écrire sous la forme $$y_i = \beta_0 + \beta_1x_i^{(1)} + \ldots + \beta_px_i^{(p)} + \varepsilon_i$$, où $y_i$ représente une observation de la variable réponse $Y$ pour l'individu $i$, $\beta_0$ l'ordonnée à l'origine, $\beta_1,...,\beta_p$ les coefficients associés à chaque variables explicatives $X_1,...,X_p$, $x_i^{(1)},...,x_i^{(p)}$ p observations (pour les p variables explicatives) pour l'individu $i$, et $e_i$ un terme d'erreur associé à cet individu $i$.
On peut voir $e_i$ comme une réalisation d'une variable aléatoire $E_i$ distribuée selon une loi normale $\mathcal{N}(0,\sigma^2)$.
En notant $$y=\begin{pmatrix}
y_1\\
\vdots\\
y_n\\
\end{pmatrix}$$, $$X=\begin{pmatrix}
1&x_1^{(1)} & \ldots & x_1^{(p)}\\
\vdots & \vdots & \ldots & \vdots \\
1 & x_n^{(1)} & \ldots & x_n^{(p)}
\end{pmatrix}$$, $$\theta=\begin{pmatrix}
\beta_0\\
\vdots\\
\beta_p\\
\end{pmatrix}$$ et \$\$e=

```{=tex}
\begin{pmatrix}
\varepsilon_0\\
\vdots\\
\varepsilon_n\\

\end{pmatrix}
```
$$, on peut réécrire le modèle précédent sous la forme $$y=X\theta+e\$\$.
Ici, $e$ est un vecteur de n réalisations indépendantes d'une variable aléatoire $E_i$ suivant une loi noramle $\mathcal{N}(0,\sigma^2)$.
Ainsi, $e$ est lui même une réalisation d'une variable aléatoire $E$ de distribution $\mathcal{N}_n(0,\sigma^2I_n)$ ($e_i$ est une observation de la variable aléatoire $E_i$ distribuée selon une loi normale $\mathcal{N}(0,\sigma^2)$).
De la même manière, $y$ est une observation de $Y=X\theta+E$ où $Y\sim\mathcal{N}_n(X\theta,\sigma^2I_n)$ ($y_i$ est l'observation de $Y_i$ distribuée selon une loi normale $\mathcal{N}((X\theta)_i,\sigma^2)$).
Ainsi, en introduisant $Y$ et $E$, le modèle précédent peut s'écrire $Y=X\theta+E$ où $\mathrm{E}\stackrel{iid}\sim\mathcal{N}_n(0,\sigma^2I_n)$.

Par définition, la réponse moyenne est de $X\theta$, plus ou moins un terme d'erreur qui en moyenne vaut 0 mais qui varie de $\sigma^2I_n$.
Ainsi, on peut écrire $Y\sim\mathcal{N}_n(X\theta,\sigma^2I_n)$.
On remarque qu'en écrivant $\mathrm{E}\stackrel{iid}\sim\mathcal{N}_n(0,\sigma^2I_n)$, toutes les erreurs ont la même variance ($\sigma^2$) et deux individus (statistiques) car tous les échantillons sont indépendants.
On verra par la suite que si cette condition d'indépendance n'est pas respectée, toutes les erreurs n'ont pas la même variance, on parle de structure de dépendance.
La dépendance entre les mesures détermine la structure de dépendance (mesures répétées dans le temps, individus groupés par une ascendance commune,...).

## 3. Dependency structures

### 3.1 Case of repeated measurements

### Repeted measurements in time

We want to evaluate the effect of different diet on weight gain in rats.
Several animals ($j$) follow each diet ($i$), and they kept the same diet accross all the experiment.
Each week animal weight ($Y_{ij}$) is measured, during T weeks.
In this case, measures are repeated accross time, such measurements are called *longitudinal data*.
To analyse this data, the temporal dependency must be taken into account, for this, the following model can be used:

$$E(Y_{ijt}) = µ + α_i + γ_t + (αγ)_{it}$$

with

$$Cov(Y_{ijt}, Y_{i'j't'}) = \left\{ 
  \begin{array}{ll}
  σ^2ρ \ \ si \ \ (i, j) = (i', j') \\
     0 \ sinon \
   \end{array}
  \right.$$

In this model, the covariance between two measurements made at times t and t' on the same model is constant, whatever the time interval between the two measurements.

**Model** A model is proposed here which takes into account the kinetic aspect of the experiment and predicts that the dependence between two measurements depends on the time interval between them.
Such a dependency cannot be represented simply as a random effect.

**Dependency structure** We assume that all measurements have the same variance

$$ V(Y_{ijt} =  σ^2) $$

and the covariance between them is

$$Cov(Y_{ijt}, Y_{i'j't'}) = \left\{ 
  \begin{array}{ll}
  σ^2ρ^{|t-t'|} \ \ si \ \ (i, j) = (i', j') \\
     0 \ sinon \
   \end{array}
  \right.$$

This structure assumes that measurements made on different animals are independent.
It is also assumed that \|ρ\| \< 1, which implies that the longer the time interval, the less correlated the tests on the same animal.
This form of covariance corresponds to an autoregressive process of order 1, generally denoted AR(1).
This model has two variance parameters: the temporal correlation ρ and the variance of each observation $σ^2$.

$$
  ψ = \left( {\begin{array}{cc}
    ρ \\
    σ^2 \\
  \end{array} } \right)
$$

Because of the independence between the measurements obtained on different animals, the variance matrix Σ also has the same diagonal block shape, but the block (R) differs.

$$  
R = \left( {\begin{array}{cc}
     σ^2 & σ^2ρ & σ^2ρ^2 & ... & σ^2ρ^{T-1}\\
     σ^2ρ & ... & ... & ... & ...\\
     σ^2ρ^2 & ... & σ^2 & ... & σ^2ρ^2\\
     ... & ... & ... & ... & σ^2ρ\\
    σ^2ρ^{T-1} & ... & σ^2ρ^2 & σ^2ρ & σ^2\\
  \end{array} } \right)
$$

We are going to use the BodyWeight dataset from the nlme package.
In this dataset, weight is measured on 16 rats every 7 days during 64 days (which gives 11 measurements for each rats).
Three diets are tested with 88 rats following diet 1 and 44 following diet 2 and 3.

The reasearch question is: Is weight gain different depending on the diet?
The model will be a mixed model with diet and time as fixed factor and individuals as random factor: weight \~ Diet \* Time \| Rat.

```{r}
head(BodyWeight, 10)
```

```{r}
time_model = gls(weight ~ Diet * Time, data = BodyWeight,correlation = corAR1(form = ~1 | Rat))
summary(time_model)
```

interpretation des results

### 3.2 Case of spatial autocorrelation

*Dependency structure* We want to take into account the dependency due to the possible spatial proximity between the sites at which the measurements were taken.

To do this, d(i, i') is the distance separating sites i and i', and the following equation is used

$$Cov(Y_i, Y_i{'}) = e^{−δ.d(i,i')}$$

As in the case of repeated measurements, there is no simple way of writing this in terms of random effects.
Moreover, since all the measurements are dependent, the matrix Σ is no longer diagonal per block and is written as :

$$
  Σ = \left( {\begin{array}{cc}
     σ^2 + γ^2 & e^{−δ.d(i,i')}& ...& e^{−δ.d(i,i')}\\
     e^{−δ.d(i,i')}& σ^2 + γ^2 & e^{−δ.d(i,i')} & \vdots\\
     \vdots & e^{−δ.d(i,i')}  & σ^2 + γ^2 & e^{−δ.d(i,i')}\\
 e^{−δ.d(i,i')}& \ldots & e^{−δ.d(i,i')} & σ^2 + γ^2\\
  \end{array} } \right)
$$

```{r, include=FALSE, echo=FALSE}
set.seed(3)  
n <- 100

loc <- data.frame(LAT = runif(n, 144, 150), LONG = runif(n,
    -26, -20))


grid <- expand.grid(LAT = seq(144, 150, l = 100), LONG = seq(-26,
    -20, l = 100))
coordinates(grid) <- ~LAT + LONG


# Set up distance matrix
distance <- as.matrix(dist(as.data.frame(loc)))  #* 1/min(distance[lower.tri(distance)])
# Generate random variable
delta <- 0.5
Vmat <- function(n, mu = 0, V = matrix(1)) {
    p <- length(mu)
    if (any(is.na(match(dim(V), p))))
        stop("Dimension problem!")
    D <- chol(V)
    t(matrix(rnorm(n * p), ncol = p) %*% D + rep(mu,
        rep(n, p)))
}
V <- Vmat(1, rep(0, n), exp(-delta * distance))
x <- rnorm(n, 30, 10)
# image(cbind(simgrid, V))
y <- 50 + 1 * x + 60 * V  #+ rnorm(n,0,1)
data.spatialCor <- data.frame(y, x, loc)
```

```{r}
data.spatialCor.glsExp <- gls(y ~ x, data = data.spatialCor,
    correlation = corExp(form = ~LAT + LONG, nugget = TRUE),
    method = "REML")
```

```{r}
summary(data.spatialCor.glsExp)
```

In spatially correlated data, variance increases with increasing distance up to a point **the sill**.
The span of distances over which points are correlated is called **the range**.

While we might expect the value of variance at a distance of zero to be zero, in reality we rarely have sampling units that approach such a small distance from one another.
The value of variance when distance is equal to zero is **the nugget**.
Typically this is the result of unexpected variability in your data that spatial patterns alone cannot account for.

Here, in our example, the value of the sill, the range and the nugget are respectively *47.68*, *1.69* and *0.12*.

## 4. Application

For this example of a mixed model application, we will use a general linear mixed model.
This is a special case of a general linear model, in which the response is quantitative and the predictor variables are both quantitative and qualitative, and the model includes random factors to take account of data dependency.
Mixed models must respect the normality of residuals and the homogeneity of variances.

## DATASET PRESENTATION AND OBJECTIVES OF THE ANALYSIS

For this work exercise, we will use data from a study performed on penguins.
This study aims to test whether species, sex,work and island influence the body mass of penguins.
In the experimental design, the data are collected from one year to the next (from 2007 to 2009), which suggests that the penguin body mass data are dependent on each other from one year to the next.
This dependency will be included in the model.
The data contains:\
- **species**: three species of penguins (Chinstrap, Adelie, or Gentoo), categorical variable\
- **island**: island name (Dream, Torgersen, or Biscoe) in the Palmer Archipelago (Antarctica), categorical variable\
- **sex**: penguin sex (female, or male), categorical variable\
- **year**: years of data collection (2007, 2008, or 2009), continuous variable\
- **body_mass_g**: body mass of the penguins (in grams), continuous variable\

The response variable is 'body_mass_g', while 'species', 'island', and 'sex' are assumed predictors.
To include data dependency, 'year' will be included as a random factor in the model.
The underlying question for this research is: do the species, island and sex drive the body mass of penguins?

### 4.1 Data import

```{r}
# Data import
df <- read.table("https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv", sep = "," , header = TRUE)
# Make sure that our variables 'species', 'island' and 'sex' are all factors in the choice.
df$species=as.factor(df$species)
df$island=as.factor(df$island)
df$sex=as.factor(df$sex)

# Check for missing values
colSums(is.na(df))
```

We can see that there are some missing values, including 2 for the response variable $Y$ 'body_mass_g' and 11 for the explanatory variable $X$ 'sex'.
We're going to delete the rows with the missing values.

```{r}
# Rows with missing values are marked.
which(is.na(df$body_mass_g), arr.ind=TRUE)
which(is.na(df$sex), arr.ind=TRUE)
# We delete the rows 4, 9, 10, 11, 12, 48, 179, 219, 257, 269, and 272.
df=df[-c(4,9,10,11,12,48,179,219,257,269,272), ]

# Check for missing values
colSums(is.na(df))
# Ok
```

### 4.2 Data exploration

Before any statistical analysis, it is ESSENTIAL to explore the data in order to avoid any errors.
Here is the list of explorations to be carried out before modelling:

1.  Check for outliers in $Y$ and the distribution of $Y$ values.
2.  If $X$ is an independent quantitative variable, check for the presence of outliers in X and the distribution of the values of X. 2b. If $X$ is a qualitative independent variable, analyse the number of levels and the number of individuals per level.
3.  Analyse the potential relationships between $Y$ and the $X_{s}$.
4.  Check for the presence of interactions between $X_{s}$.
5.  Check for collinearity between $X_{s}$.

#### 4.2.1 Outliers in $Y$ and $Y$ distribution

```{r datahist, include=TRUE, fig.height=5, fig.width=6}
par(mfrow=c(2,2))
# Boxplot
boxplot(df$body_mass_g,col='blue',ylab='Masse corporel')
# Cleveland plot
dotchart(df$body_mass_g,pch=16,col='blue',xlab='Masse corporel')
# Histogram
hist(df$body_mass_g,col='blue',xlab="Masse corporel",main="")
# Quantile-Quantile plot
qqnorm(df$body_mass_g,pch=16,col='blue',xlab='')
qqline(df$body_mass_g,col='red')
```

Here, the Boxplot and Cleveland Plot show no individuals with outliers.
The Cleveland Plot shows us that there appears to be a group of individuals with a body mass between 5000 and 6000g, while the rest is between 3000 and 4000g.
The Histogram and the QQ Plot show that $Y$ hardly follows a Normal distribution.
It is not a major issue, as the validity of the model is based on the normality of the residuals, which we will be demonstrated later.

#### 4.2.2 Outliers in $Xs$

-   For $X_s$ which are quantitative: check for outliers and distribution

No quantitative predictor here.

-   For categorical $Xs$: number of levels and number of individuals per level.

```{r datafact, include=TRUE}
# Factor Species
summary(df$species)
# Factor Island
summary(df$island)
# Factor Sex
summary(df$sex)
```

The 'species' variable has 3 levels: Adelie, Chinstrap, and Gentoo.
The number of individuals between the 3 levels is not balanced, with fewer individuals for the Chinstrap species.
The 'island' variable has 3 levels: Biscoe, Drea,m, and Torgersen.
The number of individuals between the 3 levels is not balanced, with fewer individuals for the Torgersen island.
The 'sex' variable has 2 levels: female and male.
The number of individuals per level is close to equilibrium.

#### 4.2.3 Analysis of potential relationships Y vs Xs

We can graphically analyse the possible relationships between Y and X.
Please note that this graphical analysis of the relationships between Y and X **does not predict the importance of the relationship**.
Statistical modeling is the only way to identify relationships.

```{r datagraph, include=TRUE, fig.height=4, fig.width=6}

par(mfrow=c(2,2))
# Species
plot(df$body_mass_g~df$species,pch=16,col='darkblue',xlab='Espèces',ylab='Masse corporel en g')

# Islands
plot(df$body_mass_g~df$island,pch=16,col='darkblue',xlab='Îles',ylab='Masse corporel en g')

# Sex
plot(df$body_mass_g~df$sex,pch=16,col='darkblue',xlab='Sexe',ylab='Masse corporel en g')
```

In terms of species, we can see that Gentoo has a higher body mass (between 5000 and 6000g) than the other two species (between 3000 and 4000g).
About the islands, we can see that the individuals present on Biscoe have a higher body mass (between 5000 and 6000g) than the individuals present on the other two islands (between 3000 and 4000g).
Finally, in terms of sex, males appear to have a slightly higher body mass than females.

#### 4.2.4 Analysis of possible interactions between the three independent variables

Here, we will consider the interaction between the three factors studied.
To estimate the presence of interactive effects, we develop a graphical approach.
Remember that the interaction between factors can only be tested if the factors are crossed (i.e. all the levels of one treatment are represented in all the levels of the other treatment and vice versa = a factorial design).
This point must be tested first.

```{r dataInterFac, include=TRUE, fig.height=4, fig.width=7}

# The experimental design means that the factors are cross-tabulated (all the levels of each variable are represented in all the levels of the other variables). 

# Interaction Species:Island:Sex
par(mfrow=c(1,1))
boxplot(df$body_mass_g~df$species*df$island*df$sex, 
        varwidth = TRUE, 
        xlab = "Espèces.Îles.sexe", ylab = "Body mass (g)", 
        col='blue2', main = "")
```

Explications

#### 4.2.5 Check collinearity between X

Colinearity refers to the situation in which two or more predictors of collinearity are closely related to each other.The presence of collinearity can pose problems in the context of regression, as it can be difficult to separate the individual effects of collinear variables on the response.

Here, we will test for collinearity between our 3 predictor variables:

```{r col, include=TRUE, fig.height=4, fig.width=7}
# ploting Species by Island
plot1 <- ggplot(df, aes(x=species, y=island)) +
  geom_point() +
  theme_bw()

# ploting Species by Sex
plot2 <- ggplot(df, aes(x=species, y=sex)) +
  geom_point() +
  theme_bw() 

# ploting Island by Sex
plot3 <- ggplot(df, aes(x=island, y=sex)) +
  geom_point() +
  theme_bw()

# Ploting side-by-side
marrangeGrob(list(plot1,plot2,plot3), nrow=1, ncol=3, top=NULL)
```

In our example, we can see that for the interaction between Species and Sex, there are two sex modalities per species, and for the interaction between Island and Sex, there are two sex modalities per island.
However, for the interaction between Species and islands based on, not all the islands contain all the species!
We cannot therefore test the influence of islands and species on the basis of this result.
We therefore decided to remove the Island variable from our analysis.
We will test the influence of species and sex on the body mass of penguins, always with years as a random effect.

### 4.3 Statistical analysis

#### 4.3.1 Model construction

For statistical modelling, we first analyse the full model (model containing all the independent variables to be tested).

To obtain the candidate model (a model containing only the significant terms) from the full model, we will use the **BACKWARD SELECTION METHOD**, i.e. model selection based on the significance of the terms.
In this approach, we start by creating the full model with all the variables of interest, then drop the least significant variable as long as it is not significant.
We continue by successively fitting reduced models and applying the same rule until all the remaining variables are significant.
The deletion of non-significant terms must follow the following two steps: - First, insignificant interactions are successively removed.
- Secondly, the non-significant main effects are successively removed.
A main effect is only removed if it is insignificant AND if it is not contained in a significant interaction.

In this example, we consider a measure of dependence at year level (e.g. a mass measurement made in 2009 depends on the measurement made in 2008, which in turn depends on the measurement made in 2007).
The presence of the random effect of the year will be integrated not with the lm function, but lme (from the nlme package).

```{r anova, include=TRUE}
# Full model
mod1 = lme(body_mass_g~species
              + sex
              + species:sex
              ,random=~1|year
              ,data=df)

# Then we check for significance
#anova(mod1)

#Anova Output
#            numDF denDF  F-value p-value
#(Intercept)     1   325 61569.82  <.0001
#species         2   325   758.36  <.0001
#sex             1   325   387.46  <.0001
#species:sex     2   325     8.76   2e-04
```

We can see from the anova output of our full model that each interaction and each variable is significant (\<0.05).
The full model is therefore the candidate model.

#### 4.3.2 Model's coefficients analysis

```{r coeffm, ,include=TRUE}
# Coefficients of the model
summary(mod1)

# The output is:
#Fixed effects:  body_mass_g ~ species + sex + species:sex 
#                            Value Std.Error  DF  t-value p-value
#(Intercept)              3368.836  36.21222 325 93.03036  0.0000
#speciesChinstrap          158.370  64.24029 325  2.46528  0.0142
#speciesGentoo            1310.906  54.42228 325 24.08767  0.0000
#sexmale                   674.658  51.21181 325 13.17387  0.0000
#speciesChinstrap:sexmale -262.893  90.84950 325 -2.89372  0.0041
#speciesGentoo:sexmale     130.437  76.43559 325  1.70650  0.0889


```

From this table, we can determine the coefficients of the model such that:

**Species factor**\
- $species_{Adelie}$ = 0 (the baseline of the factor Habitat) - $Species_{Chinstrap}$ = $158.370$ - $Species_{Gentoo}$ = $1310.906$

**Sex factor**\
- $Sex_{female}$ = 0 (the baseline of the factor Habitat) - $Sex_{male}$ = $674.658$

\*\*Interaction\*\
- $Species_{Chinstrap}$:$Sex_{male}$ = $-262.893$ - $Species_{Gentoo}$:$Sex_{male}$ = $130.437^{NS}$

So, the candidate model is: $$  Species = 3369 + (Adelie = 0, Chinstrap = 158, Gentoo = 1311)  + (Female = 0,\: Male = 675)$$ $$       + (Adelie_{Male} = 0, \:Chinstrap_{Male} = -263,\: Gentoo_{Male} = 130^{NS}) $$

For sake of simplicity, we can write the model depending on the sex:

The model for the *Female* pinguin is: $$ Sex_{Female} = 3369\:  + (Adelie = 0,\: Chinstrap = 158,\: Gentoo = 1311)$$

The model for the *Male* pinguin is: $$Sex_{Male} = 4043\: + (Adelie = 0,\: Chinstrap = - 105,\: Gentoo = 1441)$$

Thus, sex, species, and the interaction of these two variables (except between Male and Gentoo) do have a significant impact on penguin body mass.
For example, in Adelie penguins, the female will have a body mass of 3369g, whereas a male will have a body mass of 4043g and in Chinstrap penguins, the female will have a body mass of 3527g, whereas a male will have a body mass of 3938g.

### 4.4 Model validation

To validate the model, we need to : - Validate the normality of the residuals =\> Histogram and QQplot of the residuals - Validate the homogeneity of the variances - In addition, check for the presence of observations which would have contributed too much to the model.

#### 4.4.1 Normality of the residuals

```{r ResidNorm, include=TRUE, fig.height=3, fig.width=6}
par(mfrow=c(1,2))
# Histogram
hist(mod1$residuals,col='blue',xlab="residuals",main="Check Normality")
# Quantile-Quantile plot
qqnorm(mod1$residuals,pch=16,col='blue',xlab='')
qqline(mod1$residuals,col='red')
```

We can see that the histogram follows a normal distribution, and the quantile plot points follow the red line: the normality of the residuals is validated.

#### 4.4.2 Homogeneity of the variance

```{r Residhomo, include=TRUE, fig.height=3, fig.width=9}
par(mfrow=c(1,3))

# residuals vs fitted
plot(residuals(mod1)~fitted(mod1)
      , col='blue'
      , pch=16)
abline(h = 0)

# residuals against Species
boxplot(residuals(mod1)~ df$species, 
         varwidth = TRUE,
         ylab = "Residuals",
         xlab = "Species",
         main = "")
abline(h = 0)

# residuals against Sex
boxplot(residuals(mod1)~ df$sex, 
         varwidth = TRUE,
         ylab = "Residuals",
         xlab = "Sex",
         main = "")
abline(h = 0)

```

We can see here that for each plot, the variance of the residuals is evenly distributed around the horizontal line.
The homogeneity of the variance is validated.

#### 4.4.3 Look at influential observations

```{r Contri, include=TRUE, fig.height=4, fig.width=4}
par(mfrow = c(1, 1))
CookD(mod1,newwd=TRUE)
```

We can see that individuals 314, 315 and 325 contribute slightly more to the model, but this is not an aberrant result.

## References

The chapter was partly inspired from "Modèle mixte, modélisation de la variance" (L. Bel et al. 2016), taking some of the examples to illustrate the various aspects of mixed models.
Section 4 "Application" was adaptated from Yannick Outreman's courses on linear mixed model.
